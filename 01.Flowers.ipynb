{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de flores\n",
    "\n",
    "Bienvenidos. En este ejercicio haremos un clasificador para flores. Usaremos el siguiente dataset: http://www.robots.ox.ac.uk/~vgg/data/flowers/, que contiene 102 categorías y 8192 imágenes de flores de alta calidad. En un momento te diré cómo descargar el dataset ya editado por mi para hacerlo más fácil de manejar.\n",
    "\n",
    "Al final lograremos llegar a 99.21% de accuracy, aún cuando tenemos 102 tipos de flores diferentes. Esto es muy impresionante, sobre todo considerando que en [este paper](https://www.robots.ox.ac.uk/~vgg/research/flowers_demo/docs/Chai11.pdf) reportan 81% de accuracy, y que en [este otro](http://www.cs.huji.ac.il/~daphna/IsraeliFlowers/flower_classification.html) reportan 93.8%! \n",
    "\n",
    "Es decir, lo haremos **7.7 veces mejor** que ese último!! (0.8% vs 6.2% de error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a incluir las bibliotecas que usaremos, que son [fast.ai](https://www.fast.ai/) y [pytorch](https://pytorch.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.vision.all as fv\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo son nuestros datos?\n",
    "\n",
    "Siempre antes de empezar a trabajar en algún problema, hay que entender bien qué problema queremos resolver. En este caso, simplemente queremos clasificar flores en base a sus imágenes.\n",
    "\n",
    "Después, hay que entender los [datos](https://my.pcloud.com/publink/show?code=XZiwtw7Z15TY80dPm4b3KwmNcf1ChfphgM5y). En este caso, de las flores, pensemos que ya los descargamos, descomprimimos y están en un carpeta llamada \"flowers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/  \u001b[01;34mtmp\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalid\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "!ls flowers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posiblemente tú no tendrás tmp ni models aún. Eso se crean solitos después."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34malpine-sea-holly\u001b[0m/    \u001b[01;34mfire-lily\u001b[0m/                  \u001b[01;34mperuvian-lily\u001b[0m/\n",
      "\u001b[01;34manthurium\u001b[0m/           \u001b[01;34mfoxglove\u001b[0m/                   \u001b[01;34mpetunia\u001b[0m/\n",
      "\u001b[01;34martichoke\u001b[0m/           \u001b[01;34mfrangipani\u001b[0m/                 \u001b[01;34mpincushion-flower\u001b[0m/\n",
      "\u001b[01;34mazalea\u001b[0m/              \u001b[01;34mfritillary\u001b[0m/                 \u001b[01;34mpink-primrose\u001b[0m/\n",
      "\u001b[01;34mball-moss\u001b[0m/           \u001b[01;34mgarden-phlox\u001b[0m/               \u001b[01;34mpink-yellow-dahlia\u001b[0m/\n",
      "\u001b[01;34mballoon-flower\u001b[0m/      \u001b[01;34mgaura\u001b[0m/                      \u001b[01;34mpoinsettia\u001b[0m/\n",
      "\u001b[01;34mbarbeton-daisy\u001b[0m/      \u001b[01;34mgazania\u001b[0m/                    \u001b[01;34mprimula\u001b[0m/\n",
      "\u001b[01;34mbearded-iris\u001b[0m/        \u001b[01;34mgeranium\u001b[0m/                   \u001b[01;34mprince-of-wales-feathers\u001b[0m/\n",
      "\u001b[01;34mbee-balm\u001b[0m/            \u001b[01;34mgiant-white-arum-lily\u001b[0m/      \u001b[01;34mpurple-coneflower\u001b[0m/\n",
      "\u001b[01;34mbird-of-paradise\u001b[0m/    \u001b[01;34mglobe-flower\u001b[0m/               \u001b[01;34mred-ginger\u001b[0m/\n",
      "\u001b[01;34mbishop-of-llandaff\u001b[0m/  \u001b[01;34mglobe-thistle\u001b[0m/              \u001b[01;34mrose\u001b[0m/\n",
      "\u001b[01;34mblackberry-lily\u001b[0m/     \u001b[01;34mgrape-hyacinth\u001b[0m/             \u001b[01;34mruby-lipped-cattleya\u001b[0m/\n",
      "\u001b[01;34mblack-eyed-susan\u001b[0m/    \u001b[01;34mgreat-masterwort\u001b[0m/           \u001b[01;34msiam-tulip\u001b[0m/\n",
      "\u001b[01;34mblanket-flower\u001b[0m/      \u001b[01;34mhard-leaved-pocket-orchid\u001b[0m/  \u001b[01;34msilverbush\u001b[0m/\n",
      "\u001b[01;34mbolero-deep-blue\u001b[0m/    \u001b[01;34mhibiscus\u001b[0m/                   \u001b[01;34msnapdragon\u001b[0m/\n",
      "\u001b[01;34mbougainvillea\u001b[0m/       \u001b[01;34mhippeastrum\u001b[0m/                \u001b[01;34mspear-thistle\u001b[0m/\n",
      "\u001b[01;34mbromelia\u001b[0m/            \u001b[01;34mjapanese-anemone\u001b[0m/           \u001b[01;34mspring-crocus\u001b[0m/\n",
      "\u001b[01;34mbuttercup\u001b[0m/           \u001b[01;34mking-protea\u001b[0m/                \u001b[01;34mstemless-gentian\u001b[0m/\n",
      "\u001b[01;34mcalifornian-poppy\u001b[0m/   \u001b[01;34mlenten-rose\u001b[0m/                \u001b[01;34msunflower\u001b[0m/\n",
      "\u001b[01;34mcamellia\u001b[0m/            \u001b[01;34mlotus\u001b[0m/                      \u001b[01;34msweet-pea\u001b[0m/\n",
      "\u001b[01;34mcanna-lily\u001b[0m/          \u001b[01;34mlove-in-the-mist\u001b[0m/           \u001b[01;34msweet-william\u001b[0m/\n",
      "\u001b[01;34mcanterbury-bells\u001b[0m/    \u001b[01;34mmagnolia\u001b[0m/                   \u001b[01;34msword-lily\u001b[0m/\n",
      "\u001b[01;34mcape-flower\u001b[0m/         \u001b[01;34mmallow\u001b[0m/                     \u001b[01;34mthorn-apple\u001b[0m/\n",
      "\u001b[01;34mcarnation\u001b[0m/           \u001b[01;34mmarigold\u001b[0m/                   \u001b[01;34mtiger-lily\u001b[0m/\n",
      "\u001b[01;34mcautleya-spicata\u001b[0m/    \u001b[01;34mmexican-aster\u001b[0m/              \u001b[01;34mtoad-lily\u001b[0m/\n",
      "\u001b[01;34mclematis\u001b[0m/            \u001b[01;34mmexican-petunia\u001b[0m/            \u001b[01;34mtree-mallow\u001b[0m/\n",
      "\u001b[01;34mcolts-foot\u001b[0m/          \u001b[01;34mmonkshood\u001b[0m/                  \u001b[01;34mtree-poppy\u001b[0m/\n",
      "\u001b[01;34mcolumbine\u001b[0m/           \u001b[01;34mmoon-orchid\u001b[0m/                \u001b[01;34mtrumpet-creeper\u001b[0m/\n",
      "\u001b[01;34mcommon-dandelion\u001b[0m/    \u001b[01;34mmorning-glory\u001b[0m/              \u001b[01;34mwallflower\u001b[0m/\n",
      "\u001b[01;34mcorn-poppy\u001b[0m/          \u001b[01;34morange-dahlia\u001b[0m/              \u001b[01;34mwatercress\u001b[0m/\n",
      "\u001b[01;34mcyclamen\u001b[0m/            \u001b[01;34mosteospermum\u001b[0m/               \u001b[01;34mwater-lily\u001b[0m/\n",
      "\u001b[01;34mdaffodil\u001b[0m/            \u001b[01;34moxeye-daisy\u001b[0m/                \u001b[01;34mwild-pansy\u001b[0m/\n",
      "\u001b[01;34mdesert-rose\u001b[0m/         \u001b[01;34mpassion-flower\u001b[0m/             \u001b[01;34mwindflower\u001b[0m/\n",
      "\u001b[01;34menglish-marigold\u001b[0m/    \u001b[01;34mpelargonium\u001b[0m/                \u001b[01;34myellow-iris\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "!ls flowers/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya entendimos cómo están las imágenes y su clasificación. (Nota: el archivo original que bajas de oxford no estaba bien acomodado así, estaba en un csv y así, medio feito, yo lo convertí.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = Path(\"flowers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#102) [Path('flowers/train/stemless-gentian'),Path('flowers/train/rose'),Path('flowers/train/cautleya-spicata'),Path('flowers/train/monkshood'),Path('flowers/train/globe-flower'),Path('flowers/train/tiger-lily'),Path('flowers/train/peruvian-lily'),Path('flowers/train/bishop-of-llandaff'),Path('flowers/train/azalea'),Path('flowers/train/yellow-iris')...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(flowers/\"train\").ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#8188) [Path('flowers/train/stemless-gentian/image_05227.jpg'),Path('flowers/train/stemless-gentian/image_05216.jpg'),Path('flowers/train/stemless-gentian/image_05253.jpg'),Path('flowers/train/stemless-gentian/image_05269.jpg'),Path('flowers/train/stemless-gentian/image_05214.jpg'),Path('flowers/train/stemless-gentian/image_05273.jpg'),Path('flowers/train/stemless-gentian/image_05248.jpg'),Path('flowers/train/stemless-gentian/image_05261.jpg'),Path('flowers/train/stemless-gentian/image_05243.jpg'),Path('flowers/train/stemless-gentian/image_05263.jpg')...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = fv.get_image_files(\"flowers\"); files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 id=\"get_image_files\" class=\"doc_header\"><code>get_image_files</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/data/transforms.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4><blockquote><p><code>get_image_files</code>(<strong><code>path</code></strong>, <strong><code>recurse</code></strong>=<em><code>True</code></em>, <strong><code>folders</code></strong>=<em><code>None</code></em>)</p>\n",
       "</blockquote>\n",
       "<p>Get image files in <code>path</code> recursively, only in <code>folders</code>, if specified.</p>\n",
       "<p><a href=\"https://docs.fast.ai/data.transforms#get_image_files\" target=\"_blank\" rel=\"noreferrer noopener\">Show in docs</a></p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fv.doc(fv.get_image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abriendo los datos en fastai\n",
    "\n",
    "Dado que nuestros datos están bien acomodados en folders, vamos a crear un \"ImageDataBunch\" a partir del folder.\n",
    "\n",
    "Usaremos \"data augmentation\" (más sobre esto después), estableciendo transformadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 id=\"aug_transforms\" class=\"doc_header\"><code>aug_transforms</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/vision/augment.py#L932\" class=\"source_link\" style=\"float:right\">[source]</a></h4><blockquote><p><code>aug_transforms</code>(<strong><code>mult</code></strong>=<em><code>1.0</code></em>, <strong><code>do_flip</code></strong>=<em><code>True</code></em>, <strong><code>flip_vert</code></strong>=<em><code>False</code></em>, <strong><code>max_rotate</code></strong>=<em><code>10.0</code></em>, <strong><code>min_zoom</code></strong>=<em><code>1.0</code></em>, <strong><code>max_zoom</code></strong>=<em><code>1.1</code></em>, <strong><code>max_lighting</code></strong>=<em><code>0.2</code></em>, <strong><code>max_warp</code></strong>=<em><code>0.2</code></em>, <strong><code>p_affine</code></strong>=<em><code>0.75</code></em>, <strong><code>p_lighting</code></strong>=<em><code>0.75</code></em>, <strong><code>xtra_tfms</code></strong>=<em><code>None</code></em>, <strong><code>size</code></strong>=<em><code>None</code></em>, <strong><code>mode</code></strong>=<em><code>'bilinear'</code></em>, <strong><code>pad_mode</code></strong>=<em><code>'reflection'</code></em>, <strong><code>align_corners</code></strong>=<em><code>True</code></em>, <strong><code>batch</code></strong>=<em><code>False</code></em>, <strong><code>min_scale</code></strong>=<em><code>1.0</code></em>)</p>\n",
       "</blockquote>\n",
       "<p>Utility func to easily create a list of flip, rotate, zoom, warp, lighting transforms.</p>\n",
       "<p><a href=\"https://docs.fast.ai/vision.augment#aug_transforms\" target=\"_blank\" rel=\"noreferrer noopener\">Show in docs</a></p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fv.doc(fv.aug_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** bs = batch size. Es decir, cuántas imágenes pasa al mismo tiempo a la red neuronal. Mi tarjeta de video tiene 11GB de memoria, por es puedo poner un número relativamente grande. Si obtienes \"CUDA error: out of memory\", disminuye la batch size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos también cargar los datos \"a mano\", como se muestra en las siguientes celdas. Para más información, ver el [datablock-api](https://docs.fast.ai/data_block.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder, img_size, batch_size):\n",
    "    tfms = fv.get_transforms(flip_vert=True, max_rotate=360, max_lighting=0.3,max_zoom=1.2,max_warp=0.2) #tfms = transforms\n",
    "    data = (fv.ImageList\n",
    "            .from_folder(folder)\n",
    "            .split_by_folder()\n",
    "            .label_from_folder()\n",
    "            .transform(tfms,size=img_size)\n",
    "            .databunch(bs=batch_size))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(flowers, img_size=224, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos un objeto de tipo \"Learner\" (más sobre esto después), con arquitectura de \"resnet18\". Qué es esto? No te preocupes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = fv.cnn_learner(data, fv.models.resnet18, metrics=fai.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos *sólo de las últimas capas*\n",
    "\n",
    "Vamos a usar la función \"fit_one_cycle\", que es una manera muy rápida de entrenar (discutiremos por qué después). wd significa \"weight decay\" y es para hacer \"regularización l2\" (veremos qué significa después)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No sabemos qué taza de aprendizaje usar, y de hecho hemos estado usando la que trae por defecto (0.003). ¿Qué learning rate usamos? Pues vamos a averiguar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(); learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(4, max_lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(); learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(3,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load(\"stage1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taza de aprendizaje cíclica \n",
    "\n",
    "Como funciona el método fit_one_cycle? Se ha encontrado, experimentalmente, que ir modificando la taza de aprendizaje (learning rate) en \"ciclos\" mejora mucho el resultado final de la red. Después entenderemos la razón, pero tiene que ver con que son mejores los \"mínimos gordos\" que los \"mínimos flacos\". [Paper](https://arxiv.org/abs/1506.01186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de TODAS las capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad **no hemos estado entrenando a toda la red neuronal!!** Sólo entrenamos las últimas dos capas, y las demás estaban **pre-entrenadas en imagenet**. Por defecto, si pides una arquitectura pre-hecha, vienen \"congeladas\" las primeras capas. Vamos a entrenar con cuidado toda la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze() # ahora hay que tener cuidado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(); learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar toda la red, pero vamos a modificar las primeras capas **menos que las últimas**. Para entender por qué, veamos este paper: [Visualizing and Understading ...](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(5, max_lr=slice(1e-5,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.show_results(ds_type=fai.DatasetType.Train, rows=3) # Podemos cambiar Valid por Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretando los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Time Augmentation\n",
    "Así como hicimos data augmentation al entrenar, ¿por qué no hacerlo también al inferir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fai.accuracy(*learner.TTA())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a revisar los resultados que tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = learner.interpret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9,heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dos que predecimos mallow pero deberíamos predecir camellia... están de hecho mal clasificadas en el validation set!\n",
    "\n",
    "Esto es algo muy común, los datasets no son perfectos. Y hacer este tipo de cosas nos da herramientas para encontrar estos errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(16,16), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
